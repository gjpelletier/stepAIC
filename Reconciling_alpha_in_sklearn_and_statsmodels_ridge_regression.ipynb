{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8905ba27-fbe0-477e-9f39-515cb2689023",
   "metadata": {},
   "source": [
    "# Reconciling alpha in statsmodels and sklearn ridge regression\n",
    "\n",
    "This analysis by Paul Zivich (https://sph.unc.edu/adv_profile/paul-zivich/) explains how to get the same results of ridge regression from statsmodels and sklearn. The difference is that sklearn's Ridge function scales the input of the 'alpha' regularization term during excecution as alpha / n_samples where n_samples is the number of samples, compared with statsmodels which does not apply this scaling of the regularization parameter during execution. You can have the ridge implementations match if you re-scale the sklearn input alpha = alpha / n_samples for statsmodels. Note that this rescaling of alpha only applies to ridge regression. The sklearn and statsmodels results for Lasso regression using exactly the same alpha values for input without rescaling.\n",
    "\n",
    "Here is a link to the original post of this analysis by Paul Zivich on stackoverflow.com:  \n",
    "\n",
    "https://stackoverflow.com/questions/72260808/mismatch-between-statsmodels-and-sklearn-ridge-regression\n",
    "\n",
    "While comparing statsmodels and sklearn, Paul found that the two libraries result in different output for ridge regression. Below is an simple example of the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8651acb0-b69d-4eda-b19e-6f6320baeb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "\n",
    "np.random.seed(142131)\n",
    "\n",
    "n = 500\n",
    "d = pd.DataFrame()\n",
    "d['A'] = np.random.normal(size=n)\n",
    "d['B'] = d['A'] + np.random.normal(scale=0.25, size=n)\n",
    "d['C'] = np.random.normal(size=n)\n",
    "d['D'] = np.random.normal(size=n)\n",
    "d['intercept'] = 1\n",
    "d['Y'] = 5 - 2*d['A'] + 1*d['D'] + np.random.normal(size=n)\n",
    "\n",
    "y = np.asarray(d['Y'])\n",
    "X = np.asarray(d[['intercept', 'A', 'B', 'C', 'D']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10161b4e-05be-41ac-a2f3-417c5d80fefa",
   "metadata": {},
   "source": [
    "First, using sklearn and ridge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7821ddb3-ffe8-4874-8212-b794ef62b64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridge params from sklearn intercept and coefs: \n",
      " 4.997208595888691 [-2.00968258  0.03363013 -0.02144874  1.02895154]\n"
     ]
    }
   ],
   "source": [
    "alpha_sklearn = 1\n",
    "ridge = Ridge(alpha=alpha_sklearn, fit_intercept=True)\n",
    "ridge.fit(X=np.asarray(d[['A', 'B', 'C', 'D']]), y=y)\n",
    "print('ridge params from sklearn intercept and coefs: \\n',ridge.intercept_, ridge.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c67a3d-1652-487b-b7a8-6e44ce40dd11",
   "metadata": {},
   "source": [
    "Next, statsmodels and OLS.fit_regularized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67ea2cba-b251-4ff4-86a9-5bb8c7d461d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridge params from statsmodels: \n",
      " [ 5.01623298e+00 -6.91643749e-01 -6.39008772e-01  1.55825435e-03\n",
      "  5.51575433e-01]\n"
     ]
    }
   ],
   "source": [
    "alpha_statsmodels = np.array([0, 1., 1., 1., 1.])\n",
    "ols = sm.OLS(y, X).fit_regularized(L1_wt=0., alpha=alpha_statsmodels)\n",
    "print('ridge params from statsmodels: \\n',ols.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e90116b-3511-4cf9-a6b5-926db384a9ef",
   "metadata": {},
   "source": [
    "which outputs [5.01623, -0.69164, -0.63901, 0.00156, 0.55158]. However, since these both are implementing ridge regression, Paul expected them to be the same.\n",
    "\n",
    "Note, that neither of these penalize the intercept term (Paul checked that as a possible potential difference). Paul found that statsmodels and sklearn provide the same output for LASSO regression. Below is a demonstration with the previous data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e19d0956-4a4c-4e2e-afaa-05f32cb38b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lasso params from sklearn intercept and coefs: \n",
      " 5.014649977131442 [-1.5183174  -0.          0.          0.57799164]\n",
      "lasso params from statsmodels: \n",
      " [ 5.01464998 -1.51831729  0.          0.          0.57799166]\n"
     ]
    }
   ],
   "source": [
    "# sklearn LASSO\n",
    "alpha_sklearn = 0.5\n",
    "lasso = Lasso(alpha=alpha_sklearn, fit_intercept=True)\n",
    "lasso.fit(X=np.asarray(d[['A', 'B', 'C', 'D']]), y=y)\n",
    "print('lasso params from sklearn intercept and coefs: \\n',lasso.intercept_, lasso.coef_)\n",
    "\n",
    "# statsmodels LASSO\n",
    "alpha_statsmodels = np.array([0, 0.5, 0.5, 0.5, 0.5])\n",
    "ols = sm.OLS(y, X).fit_regularized(L1_wt=1., alpha=alpha_statsmodels)\n",
    "print('lasso params from statsmodels: \\n',ols.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f644c31-6f87-42e4-a485-34e765c3cb26",
   "metadata": {},
   "source": [
    "which both output [5.01465, -1.51832, 0., 0., 0.57799].\n",
    "\n",
    "So Paul's question is why do the estimated coefficients for ridge regression differ across implementations in sklearn and statsmodels?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d529eecd-1a1b-4c1f-b62a-a6df52c61dbc",
   "metadata": {},
   "source": [
    "After digging around a little more, Paul discovered the answer by trial and error as to why the statsmodels and sklearn ridge regression results differ. The difference is that sklearn's Ridge scales the regularization term as alpha_scaled = alpha_input / n where n is the number of observations and alpha_input is the input argument values of alpha used with sklearn. statsmodels does not apply this scaling of the regularization parameter. You can have the statsmodels and sklearn ridge implementations match if you re-scale the regularizaiton parameter used for input to sklearn when you prepare the input required for statsmodels.\n",
    "\n",
    "In other words, if you use the following input values of alpha for sklearn:\n",
    "\n",
    "alpha_sklearn = 1\n",
    "\n",
    "then you would need to use the following input of alpha=alpha_scaled when using statsmodels to get the same result:\n",
    "\n",
    "alpha_statsmodels = alpha_sklearn / n_samples\n",
    "\n",
    "where n_samples is the number of samples (n_samples = X.shape[0]).\n",
    "\n",
    "Using Paul's posted example, here is how you would have the output of ridge regression parameters match between the statsmodels and sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c303aa11-82df-4e3b-b3b6-6dea16217e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridge params from sklearn intercept and coefs: \n",
      " 4.997208595888691 [-2.00968258  0.03363013 -0.02144874  1.02895154]\n",
      "ridge params from statsmodels with alpha=alpha/n: \n",
      " [ 4.9972086  -2.00968258  0.03363013 -0.02144874  1.02895154]\n"
     ]
    }
   ],
   "source": [
    "# sklearn \n",
    "# NOTE: there is no difference from above\n",
    "alpha_sklearn = 1\n",
    "ridge = Ridge(alpha=alpha_sklearn, fit_intercept=True)\n",
    "ridge.fit(X=np.asarray(d[['A', 'B', 'C', 'D']]), y=y)\n",
    "print('ridge params from sklearn intercept and coefs: \\n',ridge.intercept_, ridge.coef_)\n",
    "\n",
    "# statsmodels\n",
    "# NOTE: going to re-scale the regularization parameter based on n observations\n",
    "n_samples = X.shape[0]\n",
    "alpha_statsmodels = np.array([0, 1., 1., 1., 1.]) / n_samples  # scaling of alpha by n\n",
    "ols = sm.OLS(y, X).fit_regularized(L1_wt=0., alpha=alpha_statsmodels)\n",
    "print('ridge params from statsmodels with alpha=alpha/n: \\n',ols.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2673021-d0af-4b40-be22-da1b589f8975",
   "metadata": {},
   "source": [
    "Now both output [ 4.99721, -2.00968, 0.03363, -0.02145, 1.02895].\n",
    "\n",
    "Paul posted this analysis in the hopes that if someone else is in the same situation trying to match resuts of ridge regression using statsmodels and sklearn, they can find the answer more easily (since Paul had not seen any discussion of this difference before). It is also noteworthy that sklearn's Ridge re-scales the tuning parameter but sklearn's Lasso does not. Paul was not able to find an explanation of this behaviour in the sklearn documentation for Ridge and LASSO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1956fb27-1a8b-4abf-922d-baef321df722",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
